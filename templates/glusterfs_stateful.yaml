apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    app: {{ template "glusterfs.fullname" . }}
  name: {{ template "glusterfs.fullname" . }}
data:
{{ (.Files.Glob "configs/gluster/*.*").AsConfig | indent 2 }}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ template "glusterfs.fullname" . }}
  labels:
    app: {{ template "glusterfs.fullname" . }}
spec:
  replicas: {{ len (splitList "," .Values.cluster.nodes) }}
  # 这个用来作为StatefulSet的pod实例服务域名，比如 glusterfs-0.glusterfs。让集群内可以固定识别
  serviceName: {{ template "glusterfs.fullname" . }}
  selector:
    matchLabels:
      app: {{ template "glusterfs.fullname" . }}
  template:
    metadata:
      labels:
        app: {{ template "glusterfs.fullname" . }}
    spec:
      # 启用host网络，让主机能直接访问。意味着节点反亲和必须设置（实际上不设置，新的pod实例也会因为端口占用而pending）
      hostNetwork: true
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: env-ficus
                    operator: Exists
                  - key: env-ficus
                    operator: In
                    values:
                      - "true"
#        {{- if .Values.cluster.hostAffinity }}
#        {{- end }}
        # 设置反亲和————“新的POD，必须不能与app:glusterfs在同一个Node，因此新的pod必须调度到另一个node”
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - {{ template "glusterfs.fullname" . }}
            topologyKey: "kubernetes.io/hostname"
      containers:
        #这部分是通过docker方式在宿主机上运行之后得到的结论
        #docker run -d --privileged=true
        #--name gluster0 --hostname=gluster0
        #-v /sobeyhive/data/glusterfs/node-0/etc:/etc/glusterfs:z --配置信息
        #-v /sobeyhive/data/glusterfs/node-0/glusterd:/var/lib/glusterd:z --核心命令
        #-v /sobeyhive/data/glusterfs/node-0/log:/var/log/glusterfs:z --日志
        #-v /sobeyhive/data/glusterfs/node-0/data:/glusterdata:z --brick数据位置
        #-v /sys/fs/cgroup:/sys/fs/cgroup:ro  --最好是把cgroup加上
        #-v /dev:/dev  --可选，最好计上
        #opendata.sobeylingyun.com/ficus/gluster/gluster-centos
        - name: {{ template "glusterfs.fullname" . }}
          image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          resources:
            limits:
              memory: 500Mi
            requests:
              memory: 200Mi
          ports:
          - name: serverport
            containerPort: 24007
            protocol: TCP
          - name: serverupdport
            containerPort: 24008
            protocol: UDP
          - name: sshd
            containerPort: 2222
            protocol: TCP
          # 获取cluster.brickports，循环映射
          {{- range .Values.cluster.brickports}}
          - name: port-{{ . }}
            containerPort: {{ . }}
            protocol: TCP
          {{- end}}
          securityContext:
            privileged: true
          command:
            - sh 
            - -c
            - "cp /init_sh/* /tmp/ && chmod +x /tmp/*.sh && /tmp/init.sh"
          env:
            - name: USER_LANGUAGE
              value: zh
            - name: USER_REGION
              value: CN
            - name: USER_TIMEZONE
              value: Asia/Shanghai
            # 用于在启动的时候，脚本中判断数量使用
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: POD_COUNT
              value: "{{ len (splitList "," .Values.cluster.nodes) }}"
            - name: POD_NODENAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            # 对应spec.serviceName
            - name: POD_SERVICE_NAME
              value: {{ template "glusterfs.fullname" . }}
            - name: POD_HOSTIP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
            - name: POD_PODIP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            # 这是给shadowadmin用来执行脚本add peer和创建卷用的
            - name: GLUSTERFS_NODES
              value: {{ .Values.cluster.nodes }}
            - name: GLUSTERFS_0
              value: {{ template "glusterfs.fullname" . }}-0
            - name: HEADLESS_GLUSTERFS_0
              value: {{ template "glusterfs.fullname" . }}-0.{{ template "glusterfs.fullname" . }}
            - name: FS_VOLUMES
              value: {{ .Values.fs.volumes }}
            - name: SHADOWADMIN_NAME
              value: {{ .Values.shadowadminname }}
            # S3服务相关
            - name: S3_ENABLE
              value: "{{ .Values.s3.enable }}"
            - name: S3_ACCOUNT
              value: {{ .Values.s3.account }}
            - name: S3_USER
              value: {{ .Values.s3.user }}
            - name: S3_PWD
              value: {{ .Values.s3.password }}
          readinessProbe:
            timeoutSeconds: 30
            initialDelaySeconds: 40
            tcpSocket:
              port: 24007
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 15
          livenessProbe:
            timeoutSeconds: 30
            initialDelaySeconds: 40
            tcpSocket:
              port: 24007
            periodSeconds: 25
            successThreshold: 1
            failureThreshold: 15
          volumeMounts:
            # 用来挂载configmap中写入的sh。 @see templates/configmap.yaml
            - mountPath: /init_sh
            # configmap.yaml中的ConfigMap是以glusterfs.fullname命名
              name: {{ template "glusterfs.fullname" . }}
            #-v /sobeyhive/data/glusterfs/node-0/etc:/etc/glusterfs:z --配置信息
            #-v /sobeyhive/data/glusterfs/node-0/glusterd:/var/lib/glusterd:z --核心命令
            #-v /sobeyhive/data/glusterfs/node-0/log:/var/log/glusterfs:z --日志
            #-v /sobeyhive/data/glusterfs/node-0/data:/glusterdata:z --brick数据位置
            #-v /sys/fs/cgroup:/sys/fs/cgroup:ro  --最好是把cgroup加上
            #-v /dev:/dev  --可选，最好计上
            - mountPath: /etc/glusterfs
              name: gluster-workspace
              # @see https://misa.gitbook.io/k8s-ocp-yaml/kubernetes-docs/2020-04-23-volume-subpathexpr
              # 在容器化的k8s环境，针对hostpath用subPath或subPathExpr，mount的主机路径一定要用一个特别的路径
              # 一定要用/var/lib/kubelet路径。 @see https://github.com/kubernetes/kubernetes/issues/61456#issuecomment-375648214 @see https://github.com/rancher/rancher/issues/14836
              subPathExpr: $(POD_NAME)/etc
            - mountPath: /var/lib/glusterd
              name: gluster-workspace
              subPathExpr: $(POD_NAME)/glusterd
            - mountPath: /var/log/glusterfs
              name: gluster-workspace
              subPathExpr: $(POD_NAME)/log
            - mountPath: /glustervolumes
              name: gluster-workspace
              subPathExpr: $(POD_NAME)/volumes
            - mountPath: /sys/fs/cgroup/
              name: sys-fs-cgroup
              readOnly: true
            - mountPath: /dev
              name: dev
      volumes:
        - name: {{ template "glusterfs.fullname" . }}
          configMap:
            defaultMode: 256
            name: {{ template "glusterfs.fullname" . }}
            optional: false
        - name: gluster-workspace
          hostPath:
            path: {{ .Values.fs.workdir }}
            type: DirectoryOrCreate
        - name: sys-fs-cgroup
          hostPath:
            path: /sys/fs/cgroup/
        - name: dev
          hostPath:
            path: /dev
#        - name: glusterfs-lvm
#          hostPath:
#            path: /run/lvm
#        - name: glusterfs-run
#          hostPath:
#            path: /run
#  S3 服务的相关POD
#  based on https://github.com/gluster/gluster-kubernetes/blob/master/deploy/kube-templates/gluster-s3-template.yaml
{{- if .Values.s3.enable }}
---
kind: Service
apiVersion: v1
metadata:
  name: glusterfs-s3
  labels:
    glusterfs: s3-service
    gluster-s3: service
spec:
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
  selector:
    glusterfs: glusterfs-s3-pod
  type: ClusterIP
  sessionAffinity: None
status:
  loadBalancer: {}
  {{- if .Values.s3.enablenodeport }}
---
apiVersion: v1
kind: Service
metadata:
  name: glusterfs-s3-nodeport
  labels:
    glusterfs: s3-service-nodeport
spec:
  type: NodePort
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
    name: s3-server-nodeport
    nodePort: {{ .Values.s3.nodeport }}
  selector:
    glusterfs: glusterfs-s3-pod
  {{- end }}
---
#  pv and pvc
apiVersion: v1
kind: PersistentVolume
metadata:
  name: glusterfs-s3-pv
spec:
  capacity:
    storage: "{{ .Values.s3.capacity }}Gi"
  accessModes:
    - ReadWriteMany
  glusterfs:
    endpoints: glusterfs-service-ep # see services.yaml
    path: {{ .Values.s3.account }}
    readOnly: false
  persistentVolumeReclaimPolicy: Retain
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gluster-s3-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: "{{ .Values.s3.capacity }}Gi"
  volumeName: "glusterfs-s3-pv"
---
#  S3-object container
kind: Deployment
apiVersion: apps/v1
metadata:
  name: gluster-s3-deployment
  labels:
    glusterfs: glusterfs-s3-pod
    gluster-s3: deployment
  annotations:
    description: Defines how to deploy gluster s3 object storage
spec:
  replicas: 1
  selector:
    matchLabels:
       glusterfs: glusterfs-s3-pod
  template:
    metadata:
      name: glusterfs-s3-pod
      labels:
        glusterfs: glusterfs-s3-pod
    spec:
      containers:
      - name: gluster-s3
        image: "{{ .Values.image.registry }}/{{ .Values.image.repositorys3 }}:latest"
        imagePullPolicy: IfNotPresent
        ports:
        - name: gluster
          containerPort: 8080
          protocol: TCP
        env:
        - name: S3_ACCOUNT
          value: "{{ .Values.s3.account }}"
        - name: S3_USER
          value: "{{ .Values.s3.user }}"
        - name: S3_PASSWORD
          value: "{{ .Values.s3.password }}"
        resources: {}
        volumeMounts:
        - name: gluster-vol1
          mountPath: "/mnt/gluster-object/{{ .Values.s3.account }}"
        - name: glusterfs-cgroup
          readOnly: true
          mountPath: "/sys/fs/cgroup"
        terminationMessagePath: "/dev/termination-log"
        securityContext:
          privileged: true
      volumes:
      - name: glusterfs-cgroup
        hostPath:
          path: "/sys/fs/cgroup"
      - name: gluster-vol1
        persistentVolumeClaim:
          claimName: gluster-s3-pvc
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: default
      serviceAccount: default
      securityContext: {}
{{- end }}